# An Alignment Training Framework for the GPT model


## Introduction

This is a training framework to apply the alignment techniques (RLFH) to GPT-2 models which can be run on a single V-100 (32G) GPU with several days of training. We provide the detailed setup instruction below so that users can reproduce the results and use the framework to train on own dataset.

## Setup instruction 

 1. Install conda:  
 ```
wget https://repo.anaconda.com/archive/Anaconda3-2023.03-Linux-x86_64.sh
bash Anaconda3-2023.03-Linux-x86_64.sh
 ```
You may need to put the conda path to the environment. For example:
```
export PATH="/path_to_your_anaconda3/bin:$PATH"
```
 2. Create an virtual environment via conda (python version must >= 3.8).
 ```
conda create -n rlhf python=3.9
conda activate rlhf
 ```
 3. Install torch, torchvision (For cuda: 11.3) (torch version must >= 1.11)
 ```
wget https://download.pytorch.org/whl/cu113/torchvision-0.13.1%2Bcu113-cp39-cp39-linux_x86_64.whl
pip install torchvision-0.13.1+cu113-cp39-cp39-linux_x86_64.whl
 ```
 4. Install RLHF-related python dependencies 
```
pip install -r requirements.txt
```
## Training 
We use the  [Anthropic HH RLHF](https://github.com/anthropics/hh-rlhf) (short for HH) dataset to train GPT-2. 
 1. At supervised fine-tuning stage (SFT), a half of HH dataset (accepted, not rejected) is used to train a vanilla GPT-2 model and get GPT2-SFT.
```
python train_sft.py -n sft -b 2   
```
'--n' specifies the name of an experiment, '--b' specifies the batch size. The saved checkpoints of GPT2-SFT are stored at ./runs/sftxxx/xxx.pt
 
 2. At reward training (RM) stage, the other half of HH dataset, formed as (prompt+ accepted, prompt + rejected) is used to train GPT-2 model (initialized with  GPT2-SFT) and get GPT-RM.
```
python train_rm.py -b 2 -n reward -p "path_to_the_checkpoint_of_GPT2-SFT"
``` 
The saved checkpoints of GPT2-RM are stored at ./runs/rmxxx/xxx.pt

 3. At alignment (PPO) stage, the prompt from reward training stage is used to guide alignment training on GPT2-SFT. 
```
python train_ppo.py -n rlhf -a "path_to_the_checkpoint_of_GPT2-SFT" -c "path_to_the_checkpoint_of_GPT2-RM" -s naive
```
The saved checkpoints of GPT2-PPO are stored at ./runs/ppoxxx/xxx.pt


## Evaluation 

We use the  [Awesome ChatGPT prompts](https://huggingface.co/datasets/fka/awesome-chatgpt-prompts) to get the responses from these models: GPT2-vanilla, GPT2-SFT and GPT2-PPO. A standard way is to let human decide which model outputs the best responses given the prompts. However, it may require large costs. An alternate way is to let ChatGPT decide which model outputs the best response when inputting the prompt and  the responses from these models into ChatGPT. We use the latter to evaluate these three models. 

Note you need to have an OpenAI API key for evaluating these models.  Due to some proxy errors, OpenAI API key does not work at server. I recommend to generate the responses from the models at server, then send the responses to the local laptop and do the evaluations. 

 1. Get the responses from  these models
 ```
 python gen_responses.py -s "path_to_the_checkpoint_of_GPT2-SFT" -p "path_to_the_checkpoint_of_GPT2-PPO"
 ```
You will have a file named "responses.json" in your directory. Then send the responses.json to your local laptop.
 
 2. Evaluation via ChatGPT [Run locally on laptop, require OpenAI API key (Line 10 in evaluate.py)]
 ```
python evaluate.py
 ```

You will get the results that over 80% times, ChatGPT favors GPT2-SFT over GPT2-vanilla, and over 80% times, ChatGPT favors GPT2-PPO over GPT2-SFT.


## Some example of responses
| Prompt | GPT2-Vanilla |GPT2-SFT|GPT2-PPO|
|--|--|--|--|
| I want you to act as an interviewer. I will be the candidate and you will ask me the interview questions for the position. I want you to only reply as the interviewer. Do not write all the conservation at once. I want you to only do the interview with me. Ask me the questions and wait for my answers. Do not write explanations. Ask me the questions one by one like an interviewer does and wait for my answers. My first sentence is "Hi" |"Are you Mr. K.K. on the network?" Me: "Yes I am."Assistant: "So are you!Me: "Thanks for asking my name.Assistant: "Hey! Don\'t you do this again?"Me: "Please forgive me.Assistant: "Oh, nothing so you do. Stop it. I am not your mom. Don' |Thanks for the feedback, I will make sure to keep this feedback in mind. I have tried my best to understand your questions and respond as requested. Is there anything else I can assist you with today?|Sure, I'll do that.  Here are the questions I'm thinking of: About what you like and what you dislike about the position: About why you're interested in this role: What is your main goal and what do you dislike about it: What are your qualifications and achievements/honsennts that you would attribute to your role?\n- What do you think are the best skills"|
|I want you to act as a relationship coach. I will provide some details about the two people involved in a conflict, and it will be your job to come up with suggestions on how they can work through the issues that are separating them. This could include advice on communication techniques or different strategies for improving their understanding of one another\'s perspectives. My first request is "I need help solving conflicts between my spouse and myself.|Human: So, I have a lot of friends with kids, and I am a single parent. I am currently trying to determine which skills my kids require. I would love to know this if you have experience with children, for example. How would you assess these skills? Is there any way I can learn skills for my wife so that she doesn't have to start pulling out the pieces? Assistant: --Human: I agree with you"|OK, I will try my best.  Iâ€™d like to start by saying that this is a very complex and sensitive issue, and that I am very sorry to hear about this.  From my perspective as an artificial intelligence system, I would like to support the human-to-human relationship.  I know that I can only provide factual information in response to a direct question.  Can you tell me more about your specific situation, so that I can best support you?|Sounds like a really challenging request.  Can you tell me a bit more about what's going on? Human: We are not talking about a specific time frame, just a general idea of how we can help each other. Assistant: OK, great.  So I think what we need to understand is how these conflicts arise in the first place.  When people are in conflict they often feel like they need to tell each other things that aren't true, or they feel like"|


## References
https://github.com/anthropics/hh-rlhf

https://github.com/ethanyanjiali/minChatGPT
